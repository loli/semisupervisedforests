\documentclass[a4paper,10pt]{article}
%\documentclass[a4paper,10pt]{scrartcl}

\usepackage[utf8]{inputenc}
\usepackage{amsmath}

\title{Schema for implementing semi-supervised forests in \textit{sklearn}}
\author{Oskar Maier}
\date{\today}

\begin{document}
\maketitle

\section{\texttt{Criterion} class hierachy and method implementation}
\noindent\texttt{Criterion} ()\\
\indent\texttt{.impurity\_improvement}\\
\\
\noindent\texttt{ClassificationCriterion} (\texttt{Criterion})\\
\indent\texttt{.\_\_cinit\_\_}\\
\indent\texttt{.init}\\
\indent\texttt{.reset}\\
\indent\texttt{.update}\\
\indent\texttt{.node\_value}\\
\\
\noindent\texttt{Entropy/Gini/SemiSupervisedEntropy} (\texttt{ClassificationCriterion})\\
\indent\texttt{.node\_impurity}\\
\indent\texttt{.children\_impurity}\\

\section{Non semi-supervised calculation of criterion in \textit{sklearn}}
\textit{sklearn} computes the impurity improvement criterion for a split in \texttt{Criterion.impurity\_improvement} using
\begin{equation}\label{eq:criterion:old}
  C=\frac{N_t}{N}\left(I(S_t) - \frac{N_{t_L}}{N_t} I(S_{t_L}) - \frac{N_{t_R}}{N_t} I(S_{t_R})\right)
\end{equation}
, where $N$ is the size of all training samples, $N_t$ the size of the training samples for the current node $t$, $N_{t_L}$ the number of samples going down the left side and $N_{t_L}$ the number going down the right side. $I(\cdot)$ is a function computing the impurity of a sample set $S$.

This impurity can be e.g. the \textit{entropy}, computed with \texttt{Entropy.node\_impurity} and \texttt{Entropy.children\_impurity} respectively. Then $I(\cdot)=E(\cdot)$, which in the case of one output (the normal case), is computed as
\begin{equation}\label{eq:entropy}
  E(S) = - \sum_{k=0}^{K-1} p(S,k) log(p(S,k))
\end{equation}
, with $K$ denoting the total number of labels and $p(S,k)$ the probability distribution of the label $k$ in smaple set $S$. In the (unusual) case of multiple outputs, their entropies are simply averaged.

\section{Base definition of semi-supervised criterion}
In the case of sem-supervised forest training, we have labeled as well as unlabeled data, which have to be treated distinctly, as the $E(\cdot)$ can not be computed for unlabeled data. Leaving aside the factor $\frac{N_t}{N}$ and following above notation, the semi-supervised criterion is defined as
\begin{multline}\label{eq:criterion:new}
  C_s=\left(E(S_{t,l}) - \frac{N_{t_L,l}}{N_{t,l}} E(S_{t_L,l}) - \frac{N_{t_R,l}}{N_{t,l}} E(S_{t_R,l})\right)\\+ \alpha\left(D(S_t) - \frac{N_{t_L}}{N_t} D(S_{t_L}) - \frac{N_{t_R}}{N_t} D(S_{t_R})\right)
\end{multline}
, with the index $l$ denoting a labeled only selection from a set. The left part of the equation corresponds to the original $C$, but only applied to the labeled part of the samples. The right part provides a way to measure the impurity of a split independent of the labels and is applied to all samples (not only the unlabeled once). $\alpha$ simply serves as a balancing term between them.

\section{Implementation troubles and reorganizing of $C_s$}
An implementation according to \eqref{eq:criterion:new} directly would (a) violate the exessive speed requirements posed to \textit{Criterion}  and (b) the direct computation of a sample sets impurity with \texttt{.node\_impurity} respectively \texttt{.children\_impurity} is required in other parts of the code.

In a perfect world, we could simply define the $I(\cdot)$ in \eqref{eq:criterion:old} as $I(\cdot)=E(\cdot)+\alpha D(\cdot)$ and avoid large changes in the code. Sadly, this does not work and reorganizing \eqref{eq:criterion:new} we reach
\begin{multline}\label{eq:criterion:new:reorganized}
  C_s=\left[E(S_{t,l}) + \alpha D(S_t)\right]\\
      - \frac{N_{t_L}}{N_t} \left[\frac{N_{t_L,l}N_t}{N_{t,l}N_{t_L}} E(S_{t_L,l}) + \alpha D(S_{t_L})\right]\\
      - \frac{N_{t_R}}{N_t} \left[\frac{N_{t_R,l}N_t}{N_{t,l}N_{t_R}} E(S_{t_R,l}) + \alpha D(S_{t_R})\right]
\end{multline}
, which comes near our desired definition except for the factor $\frac{N_{t_L,l}N_t}{N_{t,l}N_{t_L}}$. The problem here is, that the impurity of the children (left/right) nodes are computed in the child class (e.g. \texttt{Entropy}), without any knowledge of $N_t$ nor $N_{t,l}$, which depend on the parent node.

\section{Possible solution}
A new impurity calculator \texttt{SemiSupervisedEntropy} will have to provide the factors $\frac{N_{t_L,l}N_t}{N_{t,l}N_{t_L}}$ and $\frac{N_{t_R,l}N_t}{N_{t,l}N_{t_R}}$ already pre-computed to the \texttt{.children\_impurity} method. Therefore at leas \texttt{.init}, \texttt{.reset} and \texttt{.update} must be re-implemented from \texttt{ClassificationCriterion}.



\end{document}
